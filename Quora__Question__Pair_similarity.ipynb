{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora _Question _Pair_similarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m-8qRSfGWOl",
        "colab_type": "text"
      },
      "source": [
        "# Quora Question Pairs Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpT3Fu93GOvy",
        "colab_type": "text"
      },
      "source": [
        "# (1) Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyQGI1PIGTwR",
        "colab_type": "code",
        "outputId": "e93a03e2-8c67-43f3-a0fa-d4a58f9c9fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from subprocess import check_output\n",
        "# %matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "!pip install distance\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/vnd.plotly.v1+html": "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>",
            "text/html": [
              "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: distance in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKAO4OXTJxFe",
        "colab_type": "text"
      },
      "source": [
        "## (1.1)Reading data and Basic Stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5O5YVWSIuQ3",
        "colab_type": "code",
        "outputId": "81ad6726-e232-4d9c-b0ad-0e2b67284c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "print(\"Number of data points:\",df.shape[0])\n",
        "\n",
        "df.head()\n",
        "\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points: 404290\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 404290 entries, 0 to 404289\n",
            "Data columns (total 6 columns):\n",
            "id              404290 non-null int64\n",
            "qid1            404290 non-null int64\n",
            "qid2            404290 non-null int64\n",
            "question1       404289 non-null object\n",
            "question2       404288 non-null object\n",
            "is_duplicate    404290 non-null int64\n",
            "dtypes: int64(4), object(2)\n",
            "memory usage: 18.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UawCCjPHKdpf",
        "colab_type": "text"
      },
      "source": [
        "### (1.1.1) Distribution of data points among output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PurG-ZF0KaoO",
        "colab_type": "code",
        "outputId": "92ee3e28-8e3c-4d29-a8c3-e7c01231dd78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "df.groupby(\"is_duplicate\")['id'].count().plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f25ca8239b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEHCAYAAABSjBpvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npG6GhiSpm6EhSepmaEiSuhkakqRu/w8g9R8ekTTLQwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYjR_fYKKvxd",
        "colab_type": "code",
        "outputId": "133bc821-3723-4ce1-dd12-5afeecf30248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print('~> Total number of question pairs for training:\\n   {}'.format(len(df)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~> Total number of question pairs for training:\n",
            "   404290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVjiY7hXKzCU",
        "colab_type": "code",
        "outputId": "a7dd6593-7c6a-4733-897b-147e2112950a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "print('~> Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\n",
        "print('\\n~> Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(df['is_duplicate'].mean()*100, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~> Question pairs are not Similar (is_duplicate = 0):\n",
            "   63.08%\n",
            "\n",
            "~> Question pairs are Similar (is_duplicate = 1):\n",
            "   36.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJJBM8k6LaZl",
        "colab_type": "code",
        "outputId": "73647a28-b5e8-4b36-bfa2-c05398412311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df['is_duplicate'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.630802\n",
              "1    0.369198\n",
              "Name: is_duplicate, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lroWuB61L7-Y",
        "colab_type": "text"
      },
      "source": [
        "### (1.1.2) Number of unique questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSDOONTUL9_j",
        "colab_type": "code",
        "outputId": "4cdcf9cd-9026-4077-ae22-fc9be2baa29a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
        "unique_qs = len(np.unique(qids))\n",
        "qs_morethan_onetime = np.sum(qids.value_counts() > 1)\n",
        "print ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n",
        "#print len(np.unique(qids))\n",
        "\n",
        "print ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\n",
        "\n",
        "print ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n",
        "\n",
        "q_vals=qids.value_counts()\n",
        "\n",
        "q_vals=q_vals.values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of  Unique Questions are: 537933\n",
            "\n",
            "Number of unique questions that appear more than one time: 111780 (20.77953945937505%)\n",
            "\n",
            "Max number of times a single question is repeated: 157\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ2dnWX7MSh4",
        "colab_type": "code",
        "outputId": "332f4c08-812f-4885-f269-27d498712635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "x = [\"unique_questions\" , \"Repeated Questions\"]\n",
        "y =  [unique_qs , qs_morethan_onetime]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title (\"Plot representing unique and repeated questions  \")\n",
        "sns.barplot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAF2CAYAAADTMMRFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfag9mWbmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcaVDPOhMxFG",
        "colab_type": "text"
      },
      "source": [
        "### (1.1.3) Checking for Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvXYrdfMrph",
        "colab_type": "code",
        "outputId": "5c508647-42fa-47aa-9643-23c44f194563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#checking whether there are any repeated pair of questions\n",
        "pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n",
        "\n",
        "print (\"Number of duplicate questions\",(pair_duplicates).shape[0] - df.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of duplicate questions 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlM2skrsNEvF",
        "colab_type": "text"
      },
      "source": [
        "### (1.1.4) Number of occurrences of each question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKlNiUH2NF-C",
        "colab_type": "code",
        "outputId": "6c20848a-f702-49a7-8b6c-94f212532d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "plt.hist(qids.value_counts(), bins=160)\n",
        "\n",
        "plt.yscale('log', nonposy='clip')\n",
        "\n",
        "plt.title('Log-Histogram of question appearance counts')\n",
        "\n",
        "plt.xlabel('Number of occurences of question')\n",
        "\n",
        "plt.ylabel('Number of questions')\n",
        "\n",
        "print ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum number of times a single question is repeated: 157\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAJcCAYAAABAA5WYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns6gkSZIkSZKkkf0/okhPtW21WzAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poT-0TENNRzP",
        "colab_type": "text"
      },
      "source": [
        "### (1.1.5) Checking for NULL values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cPtxlbXNWWD",
        "colab_type": "code",
        "outputId": "3b133771-71c8-42a2-917e-9b99ab3e95d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "nan_rows = df[df.isnull().any(1)]\n",
        "print (nan_rows)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...  is_duplicate\n",
            "105780  105780  ...             0\n",
            "201841  201841  ...             0\n",
            "363362  363362  ...             0\n",
            "\n",
            "[3 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4EBNvbNdhC",
        "colab_type": "code",
        "outputId": "4205e96b-be79-4811-88b6-f95cf2160c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df = df.fillna('')\n",
        "nan_rows = df[df.isnull().any(1)]\n",
        "print (nan_rows)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: [id, qid1, qid2, question1, question2, is_duplicate]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUPfTUDvNnB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dEe4aKKN14G",
        "colab_type": "text"
      },
      "source": [
        "## (1.2) Basic Feature Extraction (before cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1nFSDkj2G04",
        "colab_type": "text"
      },
      "source": [
        "### Some basic feature extraction:\n",
        "#### - freq_qid1: number of times qid1 appeared in whole question1.\n",
        "#### - freq_qid2: number of times qid2 appeared in whole question2.\n",
        "#### - q1len: length of q1.\n",
        "#### - q2len: length of q1.\n",
        "#### - q1_n_words: number of words in q1.\n",
        "#### - q2_n_words: number of words in q2.\n",
        "#### - word_common: number of common words in q1 & q2.\n",
        "#### - word-total: total number of word in q1 & q2.\n",
        "#### - word_share: number of common words in q1 & q2/(total number of word in q1 & q2).\n",
        "#### - freq_q1+freq_q2: sum total of frequency of qid1 and qid2\n",
        "#### - freq_q1-freq_q2: absolute difference of frequency of qid1 and qid2 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOpQcIEgN1Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Exraction:  \n",
        "\n",
        "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
        "    df = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "else:\n",
        "    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
        "    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
        "    df['q1len'] = df['question1'].str.len() \n",
        "    df['q2len'] = df['question2'].str.len()\n",
        "    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
        "\n",
        "    def normalized_word_Common(row):\n",
        "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "        return 1.0 * len(w1 & w2)\n",
        "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
        "\n",
        "    def normalized_word_Total(row):\n",
        "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "        return 1.0 * (len(w1) + len(w2))\n",
        "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
        "\n",
        "    def normalized_word_share(row):\n",
        "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
        "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
        "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
        "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
        "\n",
        "    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
        "    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
        "\n",
        "    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMhqiBKSOXpo",
        "colab_type": "text"
      },
      "source": [
        "### (1.2.1) Analysis of some of the extracted features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-vuK-R6OWXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Minimum length of the questions in question1 : \" , min(df['q1_n_words']))\n",
        "\n",
        "print (\"Minimum length of the questions in question2 : \" , min(df['q2_n_words']))\n",
        "\n",
        "print (\"Number of Questions with minimum length [question1] :\", df[df['q1_n_words']== 1].shape[0])\n",
        "print (\"Number of Questions with minimum length [question2] :\", df[df['q2_n_words']== 1].shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25lpemgTOhUm",
        "colab_type": "text"
      },
      "source": [
        "### (1.2.1.1) Feature: word_share"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp1YGcuOOjj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtns2UcR1YPx",
        "colab_type": "text"
      },
      "source": [
        "#### The distributions for word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity.\n",
        "#### The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar).\n",
        "#### If we try to viusalize box-plot then easily see overlapping between two classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzHZ3YhcO6YO",
        "colab_type": "text"
      },
      "source": [
        "### (1.2.1.2) Feature: word_Common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNzsC_V7O3nB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxV-Eprn8X-s",
        "colab_type": "text"
      },
      "source": [
        "#### Distribution of word_common feature for both classes (similar and Non-similar Que.) highly overlapped.\n",
        "#### Here is also some correlation between word_share  & word_common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KqbIZUa702J",
        "colab_type": "text"
      },
      "source": [
        "### (1.2.1.3) Feature: freq_q1+q2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEgKO1jp7zcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'freq_q1+q2', data = df[0:])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['freq_q1+q2'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['freq_q1+q2'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuNBBGUy1bmD",
        "colab_type": "text"
      },
      "source": [
        "#### feature freq_q1+q2 not giving any difference between similar and non-similar question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QondbVT-UkP",
        "colab_type": "text"
      },
      "source": [
        "### (1.2.1.4) Feature: freq_q1-q2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYbvHbqs-alS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'freq_q1-q2', data = df[0:])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['freq_q1-q2'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['freq_q1-q2'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyL9nVbVPYE-",
        "colab_type": "text"
      },
      "source": [
        "## (1.2.2) EDA: Advanced Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqGBmiCWMlAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install  wordcloud\n",
        "\n",
        "!pip install fuzzywuzzy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-kUuHQ8PA4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from subprocess import check_output\n",
        "# %matplotlib inline\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "# This package is used for finding longest common subsequence between two strings\n",
        "# you can write your own dp code for this\n",
        "import distance\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.manifold import TSNE\n",
        "# Import the Required lib packages for WORD-Cloud generation\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from os import path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOP6KlcJReu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
        "    df = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "    df = df.fillna('')\n",
        "    df.head()\n",
        "else:\n",
        "    print(\"get df_fe_without_preprocessing_train.csv from drive or run the previous notebook\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fri-udciRj0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJLpShzvRopu",
        "colab_type": "text"
      },
      "source": [
        "# (2) Preprocessing of Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrXGGA9azfq1",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing :\n",
        "### Removing html tags, Removing Punctuations, Performing stemming, Removing Stopwords, Expanding contractions etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdCm2MHxRk3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to get results in 4decimal points.\n",
        "SAFE_DIV = 0.0001 \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "\n",
        "def preprocess(x):\n",
        "    x = str(x).lower()\n",
        "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
        "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
        "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
        "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
        "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
        "    \n",
        "    \n",
        "    porter = PorterStemmer()\n",
        "    pattern = re.compile('\\W')\n",
        "    \n",
        "    if type(x) == type(''):\n",
        "        x = re.sub(pattern, ' ', x)\n",
        "    \n",
        "    \n",
        "    if type(x) == type(''):\n",
        "        x = porter.stem(x)\n",
        "        example1 = BeautifulSoup(x)\n",
        "        x = example1.get_text()\n",
        "               \n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84i1SIOSGRk",
        "colab_type": "text"
      },
      "source": [
        "# (2.1) Advanced Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDHjyrDcMY8f",
        "colab_type": "text"
      },
      "source": [
        "### Definitions:\n",
        "\n",
        "    Token: You get a token by splitting sentence a space\n",
        "    Stop_Word : stop words as per NLTK.\n",
        "    Word : A token that is not a stop_word\n",
        "\n",
        "### Features:\n",
        "\n",
        "    cwc_min : Ratio of common_word_count to min length of word count of Q1 and Q2 without stopword\n",
        "    cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n",
        "\n",
        "\n",
        "    cwc_max : Ratio of common_word_count to max length of word count of Q1 and Q2 without stopword.\n",
        "    cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n",
        "\n",
        "\n",
        "    csc_min : Ratio of common_stop_count to min length of stop count of Q1 and Q2\n",
        "    csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n",
        "\n",
        "\n",
        "    csc_max : Ratio of common_stop_count to max length of stop count of Q1 and Q2\n",
        "    csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n",
        "\n",
        "\n",
        "    ctc_min : Ratio of common_token_count to min length of token count of Q1 and Q2\n",
        "    ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n",
        "\n",
        "\n",
        "    ctc_max : Ratio of common_token_count to max length of token count of Q1 and Q2\n",
        "    ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n",
        "\n",
        "\n",
        "    last_word_eq : Check if last word of both questions is equal or not\n",
        "    last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n",
        "\n",
        "\n",
        "    first_word_eq : Check if First word of both questions is equal or not\n",
        "    first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n",
        "\n",
        "\n",
        "    abs_len_diff : Abs. length difference\n",
        "    abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n",
        "\n",
        "\n",
        "    mean_len : Average Token Length of both Questions\n",
        "    mean_len = (len(q1_tokens) + len(q2_tokens))/2\n",
        "    \n",
        "### Fuzzywuzzy features:\n",
        " http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXHxvUvzSNQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_token_features(q1, q2):\n",
        "    token_features = [0.0]*10\n",
        "    \n",
        "    # Converting the Sentence into Tokens: \n",
        "    q1_tokens = q1.split()\n",
        "    q2_tokens = q2.split()\n",
        "\n",
        "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
        "        return token_features\n",
        "    # Get the non-stopwords in Questions\n",
        "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
        "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
        "    \n",
        "    #Get the stopwords in Questions\n",
        "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
        "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
        "    \n",
        "    # Get the common non-stopwords from Question pair\n",
        "    common_word_count = len(q1_words.intersection(q2_words))\n",
        "    \n",
        "    # Get the common stopwords from Question pair\n",
        "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
        "    \n",
        "    # Get the common Tokens from Question pair\n",
        "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
        "    \n",
        "    \n",
        "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
        "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
        "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
        "    \n",
        "    # Last word of both question is same or not\n",
        "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
        "    \n",
        "    # First word of both question is same or not\n",
        "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
        "    \n",
        "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
        "    \n",
        "    #Average Token Length of both Questions\n",
        "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
        "    return token_features\n",
        "\n",
        "# get the Longest Common sub string\n",
        "\n",
        "def get_longest_substr_ratio(a, b):\n",
        "    strs = list(distance.lcsubstrings(a, b))\n",
        "    if len(strs) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
        "      \n",
        "\n",
        "def extract_features(df):\n",
        "    # preprocessing each question\n",
        "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
        "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
        "\n",
        "    print(\"token features...\")\n",
        "    \n",
        "    # Merging Features with dataset\n",
        "    \n",
        "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    \n",
        "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
        "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
        "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
        "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
        "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
        "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
        "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
        "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
        "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
        "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
        "   \n",
        "    #Computing Fuzzy Features and Merging with Dataset\n",
        "    \n",
        "    print(\"fuzzy features..\");\n",
        "\n",
        "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8mhAH_2TQAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if os.path.isfile('nlp_features_train.csv'):\n",
        "    df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
        "    df.fillna('')\n",
        "else:\n",
        "    print(\"Extracting features for train:\")\n",
        "    df = pd.read_csv(\"train.csv\")\n",
        "    df = extract_features(df)\n",
        "    df.to_csv(\"nlp_features_train.csv\", index=False)\n",
        "df.head(2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84dyjHJTTlS5",
        "colab_type": "text"
      },
      "source": [
        "## (2.1.1) Analysis of extracted features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC6KRtblyOiV",
        "colab_type": "text"
      },
      "source": [
        "### (2.1.1.1)Plotting Words Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrDtZ6wQTdVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_duplicate = df[df['is_duplicate'] == 1]\n",
        "dfp_nonduplicate = df[df['is_duplicate'] == 0]\n",
        "\n",
        "# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\n",
        "p = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\n",
        "n = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n",
        "\n",
        "print (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\n",
        "print (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n",
        "\n",
        "#Saving the np array into a text file\n",
        "np.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\n",
        "np.savetxt('train_n.txt', n, delimiter=' ', fmt='%s',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPilo7JNUWXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# reading the text files and removing the Stop Words:\n",
        "d = path.dirname('.')\n",
        "\n",
        "textp_w = open(path.join(d, 'train_p.txt')).read()\n",
        "textn_w = open(path.join(d, 'train_n.txt')).read()\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"said\")\n",
        "stopwords.add(\"br\")\n",
        "stopwords.add(\" \")\n",
        "stopwords.remove(\"not\")\n",
        "\n",
        "stopwords.remove(\"no\")\n",
        "#stopwords.remove(\"good\")\n",
        "#stopwords.remove(\"love\")\n",
        "stopwords.remove(\"like\")\n",
        "#stopwords.remove(\"best\")\n",
        "#stopwords.remove(\"!\")\n",
        "print (\"Total number of words in duplicate pair questions :\",len(textp_w))\n",
        "print (\"Total number of words in non duplicate pair questions :\",len(textn_w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtJZ_T5hUeT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"__ Word Clouds generated from  duplicate pair question's text __\"\"\"\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\n",
        "wc.generate(textp_w)\n",
        "print (\"Word Cloud for Duplicate Question pairs\")\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaJXMJ_HUlUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n",
        "# generate word cloud\n",
        "wc.generate(textn_w)\n",
        "print (\"Word Cloud for non-Duplicate Question pairs:\")\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV47L5cCUsKP",
        "colab_type": "text"
      },
      "source": [
        "### (2.1.1.2) Pair plot of features:    ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN_gBpwwUtuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = df.shape[0]\n",
        "sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Njy2H05U0EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Distribution of the token_sort_ratio\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNi_brHpU5Tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#token_sort_ratio is interesting feature because they dont fully  overlap.\n",
        "#ideally they should fully seperated\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\n",
        "sns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4X5h5z5VT0s",
        "colab_type": "text"
      },
      "source": [
        "## (2.1.2) Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya2-TxS6VMDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 2 dimention\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "dfp_subsampled = df[0:5000]\n",
        "X = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\n",
        "y = dfp_subsampled['is_duplicate'].values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6E6b5A8VaPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne2d = TSNE(\n",
        "    n_components=2,\n",
        "    init='random', # pca\n",
        "    random_state=101,\n",
        "    method='barnes_hut',\n",
        "    n_iter=1000,\n",
        "    verbose=2,\n",
        "    angle=0.5\n",
        ").fit_transform(X)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQpBSUHVfM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n",
        "\n",
        "# draw the plot in appropriate place in the grid\n",
        "sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\n",
        "plt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK5zJDV8Vliz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne3d = TSNE(\n",
        "    n_components=3,\n",
        "    init='random', # pca\n",
        "    random_state=101,\n",
        "    method='barnes_hut',\n",
        "    n_iter=1000,\n",
        "    verbose=2,\n",
        "    angle=0.5\n",
        ").fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2H40wk_VvAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "trace1 = go.Scatter3d(\n",
        "    x=tsne3d[:,0],\n",
        "    y=tsne3d[:,1],\n",
        "    z=tsne3d[:,2],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        sizemode='diameter',\n",
        "        color = y,\n",
        "        colorscale = 'Portland',\n",
        "        colorbar = dict(title = 'duplicate'),\n",
        "        line=dict(color='rgb(255, 255, 255)'),\n",
        "        opacity=0.75\n",
        "    )\n",
        ")\n",
        "\n",
        "data=[trace1]\n",
        "layout=dict(height=800, width=800, title='3d embedding with engineered features')\n",
        "fig=dict(data=data, layout=layout)\n",
        "py.iplot(fig, filename='3DBubble')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOvKf4TpV3ER",
        "colab_type": "text"
      },
      "source": [
        "## (2.2) Featurizing text data with tfidf weighted word-vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ficCNHNlV1or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import sys\n",
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkF4Fi_gWCxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "!pip install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpqWn3Ti2XtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PngJC78zWQDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
        "df['question2'] = df['question2'].apply(lambda x: str(x))\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgITVQCUWLsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#computing tfidf weighted W2V \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# merge texts\n",
        "questions = list(df['question1']) + list(df['question2'])\n",
        "\n",
        "tfidf = TfidfVectorizer(lowercase=False, )\n",
        "tfidf.fit_transform(questions)\n",
        "\n",
        "# dict key:word and value:tf-idf score\n",
        "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgp9FP6YWefK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#spacy is the pretrained glove model.\n",
        "# https://spacy.io/usage/vectors-similarity\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "vecs1 = []\n",
        "# https://github.com/noamraph/tqdm\n",
        "# tqdm is used to print the progress bar\n",
        "for qu1 in tqdm(list(df['question1'])):\n",
        "    doc1 = nlp(qu1) \n",
        "    # 384 is the number of dimensions of vectors \n",
        "    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n",
        "    for word1 in doc1:\n",
        "        # word2vec\n",
        "        vec1 = word1.vector\n",
        "        # fetch df score\n",
        "        try:\n",
        "            idf = word2tfidf[str(word1)]\n",
        "        except:\n",
        "            idf = 0\n",
        "            \n",
        "        # compute final vec\n",
        "        mean_vec1 += vec1 * idf\n",
        "    mean_vec1 = mean_vec1.mean(axis=0)\n",
        "    vecs1.append(mean_vec1)\n",
        "df['q1_feats_m'] = list(vecs1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBuyxNOWjkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vecs2 = []\n",
        "for qu2 in tqdm(list(df['question2'])):\n",
        "    doc2 = nlp(qu2) \n",
        "    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n",
        "    for word2 in doc2:\n",
        "        # word2vec\n",
        "        vec2 = word2.vector\n",
        "        # fetch df score\n",
        "        try:\n",
        "            idf = word2tfidf[str(word2)]\n",
        "        except:\n",
        "            #print word\n",
        "            idf = 0\n",
        "        # compute final vec\n",
        "        mean_vec2 += vec2 * idf\n",
        "    mean_vec2 = mean_vec2.mean(axis=0)\n",
        "    vecs2.append(mean_vec2)\n",
        "df['q2_feats_m'] = list(vecs2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adxtLiSmW2ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
        "#nlp_features_train.csv (NLP Features)\n",
        "\n",
        "if os.path.isfile('nlp_features_train.csv'):\n",
        "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
        "else:\n",
        "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
        "\n",
        "    \n",
        "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
        "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "else:\n",
        "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VcuETrBW7RJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
        "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
        "df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_jbiJzlW-TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# dataframe of nlp features\n",
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrVcJQgmXAbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# data before preprocessing \n",
        "df2.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9XlyXnXXDFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Questions 1 tfidf weighted word2vec\n",
        "df3_q1.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb_7LDVNXFME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Questions 2 tfidf weighted word2vec\n",
        "df3_q2.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny-NgGkMXJAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(\"Number of features in nlp dataframe :\", df1.shape[1])\n",
        "print(\"Number of features in preprocessed dataframe :\", df2.shape[1])\n",
        "print(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\n",
        "print(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\n",
        "print(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x-laG5YXLQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# storing the final features to csv file\n",
        "if not os.path.isfile('final_features.csv'):\n",
        "    df3_q1['id']=df1['id']\n",
        "    df3_q2['id']=df1['id']\n",
        "    df1  = df1.merge(df2, on='id',how='left')\n",
        "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
        "    result  = df1.merge(df2, on='id',how='left')\n",
        "    result.to_csv('final_features.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgRHRkxFXkbT",
        "colab_type": "text"
      },
      "source": [
        "# (3) Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieGUTrpPXZTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing various Libraries:\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import sqlite3\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import csv\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.classification import accuracy_score, log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
        "\n",
        "!pip install mlxtend\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KkLiBVPX2Bc",
        "colab_type": "text"
      },
      "source": [
        "## (3.1) Reading data from file and storing into sql table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clsv1QYKXs-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating db file from csv\n",
        "if not os.path.isfile('train.db'):\n",
        "    disk_engine = create_engine('sqlite:///train.db')\n",
        "    start = dt.datetime.now()\n",
        "    chunksize = 180000\n",
        "    j = 0\n",
        "    index_start = 1\n",
        "    for df in pd.read_csv('final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
        "        df.index += index_start\n",
        "        j+=1\n",
        "        print('{} rows'.format(j*chunksize))\n",
        "        df.to_sql('data', disk_engine, if_exists='append')\n",
        "        index_start = df.index[-1] + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlivsNQpX8nL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_connection(db_file):\n",
        "    \"\"\" create a database connection to the SQLite database\n",
        "        specified by db_file\n",
        "    :param db_file: database file\n",
        "    :return: Connection object or None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "        return conn\n",
        "    except Error as e:\n",
        "        print(e)\n",
        " \n",
        "    return None\n",
        "\n",
        "\n",
        "def checkTableExists(dbcon):\n",
        "    cursr = dbcon.cursor()\n",
        "    str = \"select name from sqlite_master where type='table'\"\n",
        "    table_names = cursr.execute(str)\n",
        "    print(\"Tables in the databse:\")\n",
        "    tables =table_names.fetchall() \n",
        "    print(tables[0][0])\n",
        "    return(len(tables))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYuS4rrbYCfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "read_db = 'train.db'\n",
        "conn_r = create_connection(read_db)\n",
        "checkTableExists(conn_r)\n",
        "conn_r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwvuohPcYJSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try to sample data according to the computing power\n",
        "if os.path.isfile(read_db):\n",
        "    conn_r = create_connection(read_db)\n",
        "    if conn_r is not None:\n",
        "        # for selecting first 1M rows\n",
        "        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n",
        "        \n",
        "        # for selecting random points\n",
        "        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n",
        "        conn_r.commit()\n",
        "        conn_r.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9opOaEJyYOU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# remove the first row \n",
        "data.drop(data.index[0], inplace=True)\n",
        "y_true = data['is_duplicate']\n",
        "data.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E07n0TQYXbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwqdb8umYaqO",
        "colab_type": "text"
      },
      "source": [
        "## (3.2) Converting strings to numerics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZBsYqmPYT_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = list(data.columns)\n",
        "for i in cols:\n",
        "    data[i] = data[i].apply(pd.to_numeric)\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDQoGcn7Yuyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = list(map(int, y_true.values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SrvynyQYqQR",
        "colab_type": "text"
      },
      "source": [
        "## (3.3) Random train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjbQgklmYisJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SF73Xh3Y-Ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of data points in train data :\",X_train.shape)\n",
        "print(\"Number of data points in test data :\",X_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak88WIGwY2Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
        "train_distr = Counter(y_train)\n",
        "train_len = len(y_train)\n",
        "print(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n",
        "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
        "test_distr = Counter(y_test)\n",
        "test_len = len(y_test)\n",
        "print(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgfzkNJ2ZCtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# This function plots the confusion matrices given y_i, y_i_hat.\n",
        "def plot_confusion_matrix(test_y, predict_y):\n",
        "    C = confusion_matrix(test_y, predict_y)\n",
        "    \n",
        "    A =(((C.T)/(C.sum(axis=1))).T)\n",
        "    \n",
        "    plt.figure(figsize=(20,4))\n",
        "    \n",
        "    labels = [1,2]\n",
        "    \n",
        "    # representing A in heatmap format\n",
        "    cmap=sns.light_palette(\"blue\")\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Precision matrix\")\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    # representing B in heatmap format\n",
        "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Recall matrix\")\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQf7MfvpZYzc",
        "colab_type": "text"
      },
      "source": [
        "## (3.4) Building a random model (Finding worst-case log-loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP2jJiXVZQ8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_y = np.zeros((test_len,2))\n",
        "for i in range(test_len):\n",
        "    rand_probs = np.random.rand(1,2)\n",
        "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
        "print(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n",
        "\n",
        "predicted_y =np.argmax(predicted_y, axis=1)\n",
        "plot_confusion_matrix(y_test, predicted_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h6C7js4Zg9y",
        "colab_type": "text"
      },
      "source": [
        "## (3.5) Logistic Regression with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqQQRA_sZVM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_test)\n",
        "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test, predicted_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy98voCJkLXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scale = StandardScaler()\n",
        "X_train_std = scale.fit_transform(X_train)\n",
        "X_test_std = scale.transform(X_test)\n",
        "\n",
        "\n",
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(X_train_std, y_train)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_std, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_test_std)\n",
        "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(X_train_std, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_std, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_std)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_test_std)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms_v88AIk_O_",
        "colab_type": "text"
      },
      "source": [
        "## (3.6) Linear SVM with hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkJpmbzzktIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_test)\n",
        "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test, predicted_y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16EHjjIlOQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scale = StandardScaler()\n",
        "X_train_std_std = scale.fit_transform(X_train)\n",
        "X_test_std_std = scale.transform(X_test)\n",
        "\n",
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
        "    clf.fit(X_train_std, y_train)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_std, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_test_std)\n",
        "    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
        "clf.fit(X_train_std, y_train)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_std, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_std)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_test_std)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test, predicted_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxzv33P0lkoO",
        "colab_type": "text"
      },
      "source": [
        "## (3.7) Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCfAaHBhle-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "estimators = [100,150,200,300,400,600,800,1000]#parameter tuning of n_estimators\n",
        "test_scores = []\n",
        "train_scores = []\n",
        "for i in estimators:\n",
        "    clf = RandomForestClassifier(n_estimators=i,n_jobs=-1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    predict_y = clf.predict_proba(X_train)\n",
        "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
        "    train_scores.append(log_loss_train)\n",
        "    predict_y = clf.predict_proba(X_test)\n",
        "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
        "    test_scores.append(log_loss_test)\n",
        "    print('estimators = ',i,'\\Train Log Loss ',log_loss_train,'\\Test Log Loss ',log_loss_test,'\\Loss difference between Test and Train',(log_loss_test-log_loss_train))\n",
        "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
        "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
        "plt.xlabel('estimators')\n",
        "plt.ylabel('Log Loss')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60WTpAl3ltcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "max_depth = [5,10,12,15,20,25,50]#parameter tuning of max_depth\n",
        "test_scores = []\n",
        "train_scores = []\n",
        "for i in max_depth:\n",
        "    clf = RandomForestClassifier(n_estimators=200,max_depth=i,n_jobs=-1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    predict_y = clf.predict_proba(X_train)\n",
        "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
        "    train_scores.append(log_loss_train)\n",
        "    predict_y = clf.predict_proba(X_test)\n",
        "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
        "    test_scores.append(log_loss_test)\n",
        "    print('max_depth = ',i,'\\Train Log Loss ',log_loss_train,'\\Test Log Loss ',log_loss_test,'\\Loss difference between Test and Train',(log_loss_test-log_loss_train))\n",
        "plt.plot(max_depth,train_scores,label='Train Log Loss')\n",
        "plt.plot(max_depth,test_scores,label='Test Log Loss')\n",
        "plt.xlabel('estimators') \n",
        "plt.ylabel('Log Loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orU537X7l6oN",
        "colab_type": "text"
      },
      "source": [
        "## (3.8) XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5beSgS_zlzQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "params = {}\n",
        "params['objective'] = 'binary:logistic'\n",
        "params['eval_metric'] = 'logloss'\n",
        "params['eta'] = 0.02\n",
        "params['max_depth'] = 4\n",
        "\n",
        "d_train = xgb.DMatrix(X_train, label=y_train)\n",
        "d_test = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "watchlist = [(d_train, 'train'), (d_test, 'valid')]\n",
        "\n",
        "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n",
        "\n",
        "xgdmat = xgb.DMatrix(X_train,y_train)\n",
        "predict_y = bst.predict(d_test)\n",
        "print(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMeNonopmES3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "predicted_y =np.array(predict_y>0.5,dtype=int)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxWLjUd8mx4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import xgboost as xgb\n",
        "estimators = [100,200,250,300,350,400,600]  #parameter tuning of estimators (no. of base learners.)\n",
        "test_scores = []\n",
        "train_scores = []\n",
        "for i in estimators:\n",
        "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=i,n_jobs=-1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    predict_y = clf.predict_proba(X_train)\n",
        "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
        "    train_scores.append(log_loss_train)\n",
        "    predict_y = clf.predict_proba(X_test)\n",
        "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
        "    test_scores.append(log_loss_test)\n",
        "    print('estimators = ',i,'\\Train Log Loss ',log_loss_train,'\\Test Log Loss ',log_loss_test,'\\Loss difference between Test and Train',(log_loss_test-log_loss_train))\n",
        "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
        "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
        "plt.xlabel('estimators')\n",
        "plt.ylabel('Log Loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWBk0HBqnI_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_scores = []\n",
        "train_scores = []\n",
        "etas = [0.05,0.1,0.15,0.2,0.25,0.3]  #parameter tuning of learning rate.\n",
        "for i in etas:\n",
        "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=i,n_estimators=350,n_jobs=-1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    predict_y = clf.predict_proba(X_train)\n",
        "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
        "    train_scores.append(log_loss_train)\n",
        "    predict_y = clf.predict_proba(X_test)\n",
        "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
        "    test_scores.append(log_loss_test)\n",
        "    print('etas = ',i,'\\Train Log Loss ',log_loss_train,'\\Test Log Loss ',log_loss_test,'\\Loss difference between Test and Train',(log_loss_test-log_loss_train))\n",
        "plt.plot(etas,train_scores,label='Train Log Loss')\n",
        "plt.plot(etas,test_scores,label='Test Log Loss')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Log Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3wig0fZnI0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZWiIcBDm2wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores = []\n",
        "train_scores = []\n",
        "alpha = [0.5,1,5,10,50,100,150]#parameter tuning of alpha\n",
        "for i in alpha:\n",
        "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.15,n_estimators=350,reg_alpha=i,n_jobs=-1)\n",
        "    clf.fit(X_train,y_train)\n",
        "    predict_y = clf.predict_proba(X_train)\n",
        "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
        "    train_scores.append(log_loss_train)\n",
        "    predict_y = clf.predict_proba(X_test)\n",
        "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
        "    test_scores.append(log_loss_test)\n",
        "    print('reg_alpha = ',i,'\\Train Log Loss ',log_loss_train,'\\Test Log Loss ',log_loss_test,'\\Loss difference between Test and Train',(log_loss_test-log_loss_train))\n",
        "plt.plot(alpha,train_scores,label='Train Log Loss')\n",
        "plt.plot(alpha,test_scores,label='Test Log Loss')\n",
        "plt.xlabel('reg_alpha')\n",
        "plt.ylabel('Log Loss')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlSGUp4Ym7oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.15,n_estimators=350,n_jobs=-1)\n",
        "clf.fit(X_train,y_train)\n",
        "predict_y = clf.predict_proba(X_test)\n",
        "print(\"The test log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "plot_confusion_matrix(y_test, predicted_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqFBkK_9sjT",
        "colab_type": "text"
      },
      "source": [
        "## (4) Converting Text data in TF-IDF vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD6sel45nOcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqdIFvW9oY5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKhp5JzTocZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile('nlp_features_train.csv'):\n",
        "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
        "else:\n",
        "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
        "\n",
        "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
        "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
        "else:\n",
        "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzUxavzjoowS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = dfnlp.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
        "df3 = dfnlp[['id','question1','question2']]\n",
        "duplicate = dfnlp.is_duplicate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsKrp2gIosQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3 = df3.fillna(' ')\n",
        "df4 = pd.DataFrame()\n",
        "df4['Text'] = df3.question1 + ' ' + df3.question2\n",
        "df4['id'] = df3.id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8zuIsiHovxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combining question1 and question2, then getting Tf-Idf\n",
        "df2['id']=df1['id']\n",
        "df4['id']=df1['id']\n",
        "df5  = df1.merge(df2, on='id',how='left')\n",
        "final  = df5.merge(df4, on='id',how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hqo5s3ooynF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final = final.drop('id',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKZtdmtvo7vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train test split\n",
        "X_train_tfidf,X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(final,duplicate,  test_size=0.3,random_state=24)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGrkiVmpDHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(ngram_range=(1,4),max_features=200000,min_df=0.000032)\n",
        "train_tfidf = tfidf_vect.fit_transform(X_train_tfidf.Text)\n",
        "test_tfidf = tfidf_vect.transform(X_test_tfidf.Text)\n",
        "print('length of Tfidf features',len(tfidf_vect.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqsc_0wWpIvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tfidf = X_train_tfidf.drop('Text',axis=1)\n",
        "X_test_tfidf = X_test_tfidf.drop('Text',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g8dRL_5pKmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import hstack\n",
        "X_train1 = hstack((X_train_tfidf.values,train_tfidf))\n",
        "X_test1 = hstack((X_test_tfidf.values,test_tfidf))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESr0UhfApQMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"standardized\"\"\"\n",
        "\n",
        "scale = StandardScaler(with_mean=False)\n",
        "X_train_stdn = scale.fit_transform(X_train_tfidf)\n",
        "X_test_stdn = scale.transform(X_test_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyCLKmB5pSTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from scipy.sparse import hstack\n",
        "X_train_scale = hstack((X_train_stdn,train_tfidf))\n",
        "X_test_scale = hstack((X_test_stdn,test_tfidf))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mINbsmpWb6",
        "colab_type": "text"
      },
      "source": [
        "## (4.1) Logistic Regression with hyperparameter tuning(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHP3BFXIoT2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(X_train1, y_train_tfidf)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train1, y_train_tfidf)\n",
        "    predict_y = sig_clf.predict_proba(X_test1)\n",
        "    log_error_array.append(log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(X_train1, y_train_tfidf)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train1, y_train_tfidf)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train1)\n",
        "log_loss_train_logistic=log_loss(y_train_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss_train_logistic)\n",
        "predict_y = sig_clf.predict_proba(X_test1)\n",
        "log_loss_test_logistic=log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss_test_logistic)\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test_tfidf, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQJ-QmkupkRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
        "    clf.fit(X_train_scale, y_train_tfidf)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_scale, y_train_tfidf)\n",
        "    predict_y = sig_clf.predict_proba(X_test_scale)\n",
        "    log_error_array.append(log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
        "clf.fit(X_train_scale, y_train_tfidf)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_scale, y_train_tfidf)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_scale)\n",
        "log_loss_train_logistic_scaled=log_loss(y_train_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss_train_logistic_scaled)\n",
        "predict_y = sig_clf.predict_proba(X_test_scale)\n",
        "log_loss_test_logistic_scaled=log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss_test_logistic_scaled)\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test_tfidf, predicted_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iW5wyOVqT-Q",
        "colab_type": "text"
      },
      "source": [
        "## (4.2) Linear SVM with hyperparameter tuning(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcTtVbV9qRsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSQyTlkp-Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
        "    clf.fit(X_train1, y_train_tfidf)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train1, y_train_tfidf)\n",
        "    predict_y = sig_clf.predict_proba(X_test1)\n",
        "    log_error_array.append(log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
        "clf.fit(X_train1, y_train_tfidf)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train1, y_train_tfidf)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train1)\n",
        "log_loss_train_svm=log_loss(y_train_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss_train_svm)\n",
        "predict_y = sig_clf.predict_proba(X_test1)\n",
        "log_loss_test_svm=log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss_test_svm)\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test_tfidf, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyCuCgpPqgHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"with standardized scaled dataset\"\"\"\n",
        "\n",
        "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
        "\n",
        "log_error_array=[]\n",
        "for i in alpha:\n",
        "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
        "    clf.fit(X_train_scale, y_train_tfidf)\n",
        "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_scale, y_train_tfidf)\n",
        "    predict_y = sig_clf.predict_proba(X_test_scale)\n",
        "    log_error_array.append(log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(log_error_array)\n",
        "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
        "clf.fit(X_train_scale, y_train_tfidf)\n",
        "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_scale, y_train_tfidf)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_scale)\n",
        "log_loss_train_svm_scaled=log_loss(y_train_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss_train_svm_scaled)\n",
        "predict_y = sig_clf.predict_proba(X_test_scale)\n",
        "log_loss_test_svm_scaled=log_loss(y_test_tfidf, predict_y, labels=clf.classes_, eps=1e-15)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss_test_svm_scaled)\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "print(\"Total number of data points :\", len(predicted_y))\n",
        "plot_confusion_matrix(y_test_tfidf, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzEaOfhqxGZ",
        "colab_type": "text"
      },
      "source": [
        "## (4.3) XGBoost(tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEHk_7X_qvPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njBkJRnSqsbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import random,math\n",
        "from random import randint\n",
        "from random import uniform\n",
        "\n",
        "param_dist = {\"max_depth\": [randint(2,5)],\n",
        "              \"learning_rate\":[uniform(0,0.25)],\n",
        "              \"n_estimators\":[randint(100,600)],\n",
        "              \"min_child_weight\": [randint(2, 8)],\n",
        "              \"gamma\": [uniform(0,4)],\n",
        "              \"subsample\":[uniform(0.3,0.7)],\n",
        "              \"colsample_bytree\": [uniform(0.3,0.7)],\n",
        "              \"reg_alpha\":[uniform(100,300)],\n",
        "              \"reg_lambda\":[uniform(100,300)]}\n",
        "\n",
        "xgb_classifier= xgb.XGBClassifier(n_jobs=-1,random_state=25)\n",
        "model_rs_xgb = RandomizedSearchCV(xgb_classifier, param_distributions=param_dist,n_iter=30,scoring='neg_log_loss',cv=10,n_jobs=-1)\n",
        "model_rs_xgb.fit(X_train_scale,y_train_tfidf)\n",
        "pickle.dump(model_rs_xgb,open('model_rs_xgb.p','wb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uxBK4-jq-HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rs_xgb.best_params_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAcLr_tcrCC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "clf = xgb.XGBClassifier(max_depth=4,learning_rate=0.18739843716109036,n_estimators=455,min_child_weight=3,\n",
        "                        reg_alpha=147.48355681277022,reg_lambda=298.24507830962364,\n",
        "                        gamma=0.08991146971048991,colsample_bytree=0.3395793695766003,n_jobs=-1)\n",
        "clf.fit(X_train_scale,y_train_tfidf)\n",
        "y_pred_test=clf.predict_proba(X_test_scale)\n",
        "y_pred_train=clf.predict_proba(X_train_scale)\n",
        "log_loss_train_xgb = log_loss(y_train_tfidf, y_pred_train, eps=1e-15)\n",
        "log_loss_test_xgb=log_loss(y_test_tfidf,y_pred_test,eps=1e-15)\n",
        "print('Train log loss = ',log_loss_train_xgb,' Test log loss = ',log_loss_test_xgb)\n",
        "predicted_y =np.argmax(predict_y,axis=1)\n",
        "plot_confusion_matrix(y_test_tfidf, predicted_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyefzIpyrG4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install PrettyTable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30pnlNccrOUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from prettytable import PrettyTable    \n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model\",\"vectorizer\",\"logloss_train\",\"logloss_test\",\"logloss_train(with standardized)\",\"logloss_test(with standardized)\"]\n",
        "x.add_row(['Random model','TFIDF w2vec', '---'  , '0.8851460427625617','---','---'])\n",
        "x.add_row(['Logistic regression','TFIDF w2vec','0.50036000082','0.505431586320','0.403156873165','0.413104155793449'])\n",
        "x.add_row(['Linear SVM','TFIDF w2vec','0.478578596724142','0.4876647905346559','0.4180234067450405','0.4268263608040993'])\n",
        "x.add_row(['XGBOOST','TFIDF w2vec', '0.341883'  , '0.354821','---','---'])\n",
        "x.add_row(['Logistic regression','TFIDF ',log_loss_train_logistic   , log_loss_test_logistic , log_loss_train_logistic_scaled,log_loss_test_logistic_scaled])\n",
        "x.add_row(['Linear SVM','TFIDF',  log_loss_train_svm  , log_loss_test_svm,log_loss_train_svm_scaled,log_loss_test_svm_scaled])\n",
        "x.add_row(['XGBOOST_tuned','TFIDF ',log_loss_train_xgb,log_loss_test_xgb,'---','---'])\n",
        "\n",
        "print(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdk3mGnB8qoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "data = x.get_string()\n",
        "\n",
        "with open('Quora.txt', 'w') as f:\n",
        "    f.write(data)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}